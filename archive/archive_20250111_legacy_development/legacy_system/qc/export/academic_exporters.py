#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Additional Academic Export Methods for QCA Data Exporter
Extends the main data_exporter.py with SPSS, Word, and GraphML functionality
"""

import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from collections import defaultdict

logger = logging.getLogger(__name__)

try:
    from docx import Document
    from docx.shared import Inches
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    logger.warning("python-docx not available - Word export will be unavailable")

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    logger.warning("networkx not available - GraphML export will be unavailable")


class AcademicExporters:
    """Additional academic export methods for the QCA data exporter"""
    
    def __init__(self, output_dir: Union[str, Path] = "data/exports"):
        """
        Initialize academic exporters
        
        Args:
            output_dir: Directory to save export files
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def export_spss_syntax(self, interviews: List[Dict[str, Any]], output_file: str = None) -> str:
        """
        Export SPSS syntax file for importing and analyzing QCA data
        
        Args:
            interviews: List of interview data dictionaries
            output_file: Output filename (auto-generated if None)
            
        Returns:
            Path to generated SPSS syntax file
        """
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"qca_spss_import_{timestamp}.sps"
            
        output_path = self.output_dir / output_file
        
        # Get all unique codes for variable definitions
        all_codes = set()
        for interview in interviews:
            for quote in interview.get('quotes', []):
                all_codes.update(quote.get('code_names', []))
        
        # Create corresponding CSV data file
        csv_file = f"qca_spss_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        self._create_spss_csv_data(interviews, csv_file)
        
        spss_syntax = f'''* SPSS Syntax for QCA Data Import and Analysis
* Generated by QCA Pipeline on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
* Data source: {csv_file}

* Import CSV data
GET DATA
  /TYPE=TXT
  /FILE="{csv_file}"
  /ENCODING='UTF8'
  /DELCASE=LINE
  /DELIMITERS=","
  /QUALIFIER='"'
  /ARRANGEMENT=DELIMITED
  /FIRSTCASE=2
  /VARIABLES=
    quote_id A50
    interview_id A50
    speaker_name A100
    quote_text A1000
    location_start F8.0
    location_end F8.0
    code_count F8.0
    codes_list A500
'''
        
        # Add binary code variables
        for code in sorted(all_codes):
            # Create SPSS-compatible variable names
            spss_var_name = f"code_{code.lower().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')}"
            # Limit to 64 characters (SPSS variable name limit)
            spss_var_name = spss_var_name[:64]
            spss_syntax += f"    {spss_var_name} F1.0\n"
        
        spss_syntax += '''\n.

* Define variable labels
VARIABLE LABELS
    quote_id "Unique Quote Identifier"
    interview_id "Interview Identifier" 
    speaker_name "Speaker Name"
    quote_text "Quote Text Content"
    location_start "Quote Start Position"
    location_end "Quote End Position"
    code_count "Number of Codes Applied"
    codes_list "List of Applied Codes"
'''
        
        # Add labels for code variables
        for code in sorted(all_codes):
            spss_var_name = f"code_{code.lower().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')}"
            spss_var_name = spss_var_name[:64]
            spss_syntax += f'    {spss_var_name} "Code: {code}"\n'
        
        spss_syntax += '''\n.

* Define value labels for binary code variables
'''
        
        for code in sorted(all_codes):
            spss_var_name = f"code_{code.lower().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')}"
            spss_var_name = spss_var_name[:64]
            spss_syntax += f'''VALUE LABELS {spss_var_name}
    0 "Code Not Present"
    1 "Code Present".
'''
        
        spss_syntax += f'''\n* Basic frequency analysis
FREQUENCIES VARIABLES=code_count
  /STATISTICS=MEAN STDDEV MIN MAX
  /HISTOGRAM.

* Cross-tabulation analysis example
* (Uncomment and modify as needed for your analysis)
* CROSSTABS
*   /TABLES={' BY '.join(list(sorted(all_codes))[:3][:64])}
*   /FORMAT=AVALUE TABLES
*   /STATISTICS=CHISQ
*   /CELLS=COUNT EXPECTED ROW COLUMN TOTAL.

* Correlation analysis for code co-occurrence
CORRELATIONS
  /VARIABLES=''' + ' '.join([f"code_{code.lower().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')}"[:64] for code in sorted(all_codes)][:10]) + '''
  /PRINT=TWOTAIL NOSIG
  /STATISTICS DESCRIPTIVES.

* Save processed dataset
SAVE OUTFILE='qca_analysis.sav'
  /COMPRESSED.

* Analysis suggestions:
* 1. Use CROSSTABS for examining relationships between codes
* 2. Use CORRELATIONS for code co-occurrence patterns
* 3. Use FREQUENCIES for code distribution analysis
* 4. Consider FACTOR analysis for dimensionality reduction
* 5. Use CLUSTER for grouping similar interviews or quotes

* End of syntax
'''
        
        # Write SPSS syntax file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(spss_syntax)
        
        logger.info(f"Exported SPSS syntax file: {output_path}")
        logger.info(f"Data file created: {csv_file}")
        return str(output_path)
    
    def _create_spss_csv_data(self, interviews: List[Dict[str, Any]], csv_file: str) -> str:
        """Create CSV data file for SPSS import"""
        import csv
        
        output_path = self.output_dir / csv_file
        
        # Get all unique codes
        all_codes = set()
        for interview in interviews:
            for quote in interview.get('quotes', []):
                all_codes.update(quote.get('code_names', []))
        
        # Prepare data rows
        data_rows = []
        for interview in interviews:
            interview_id = interview.get('interview_id', 'Unknown')
            for quote in interview.get('quotes', []):
                row = {
                    'quote_id': str(quote.get('id', '')),
                    'interview_id': str(interview_id),
                    'speaker_name': str(quote.get('speaker', {}).get('name', '')),
                    'quote_text': str(quote.get('text', '')),
                    'location_start': int(quote.get('location_start', 0) or 0),
                    'location_end': int(quote.get('location_end', 0) or 0),
                    'code_count': len(quote.get('code_names', [])),
                    'codes_list': '|'.join(quote.get('code_names', [])),
                }
                
                # Add binary indicators for each code
                for code in sorted(all_codes):
                    spss_var_name = f"code_{code.lower().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')}"
                    spss_var_name = spss_var_name[:64]
                    row[spss_var_name] = 1 if code in quote.get('code_names', []) else 0
                
                data_rows.append(row)
        
        # Write CSV file
        if data_rows:
            fieldnames = data_rows[0].keys()
            with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(data_rows)
        
        return str(output_path)
    
    def export_word_report(self, analysis_data: Dict[str, Any], output_file: str = None) -> str:
        """
        Export analysis results as Microsoft Word document
        
        Args:
            analysis_data: Complete analysis data
            output_file: Output filename (auto-generated if None)
            
        Returns:
            Path to generated Word file
        """
        if not DOCX_AVAILABLE:
            raise ImportError("python-docx required for Word export")
        
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"qca_analysis_report_{timestamp}.docx"
            
        output_path = self.output_dir / output_file
        
        interviews = analysis_data.get('interviews', [])
        taxonomy = analysis_data.get('taxonomy', {})
        
        # Calculate summary statistics
        total_quotes = sum(len(interview.get('quotes', [])) for interview in interviews)
        total_codes = len(taxonomy.get('codes', []))
        
        # Create Word document
        doc = Document()
        
        # Title and metadata
        title = doc.add_heading('Qualitative Coding Analysis Report', 0)
        doc.add_paragraph(f'Generated on: {datetime.now().strftime("%B %d, %Y")}')
        doc.add_paragraph('Generated by: QCA Pipeline System')
        
        # Executive Summary
        doc.add_heading('Executive Summary', level=1)
        summary = doc.add_paragraph(
            f'This report presents comprehensive results from qualitative coding analysis '
            f'of {len(interviews)} interviews. The analysis identified {total_codes} thematic codes '
            f'across {total_quotes} coded quotes, providing insights into key patterns and themes '
            f'within the data.'
        )
        
        # Analysis Overview
        doc.add_heading('Analysis Overview', level=1)
        overview_table = doc.add_table(rows=1, cols=2)
        overview_table.style = 'Table Grid'
        hdr_cells = overview_table.rows[0].cells
        hdr_cells[0].text = 'Metric'
        hdr_cells[1].text = 'Value'
        
        metrics = [
            ('Total Interviews', str(len(interviews))),
            ('Total Quotes', str(total_quotes)),
            ('Total Codes', str(total_codes)),
            ('Average Quotes per Interview', f"{total_quotes / max(len(interviews), 1):.1f}"),
            ('Analysis Date', datetime.now().strftime('%Y-%m-%d'))
        ]
        
        for metric, value in metrics:
            row_cells = overview_table.add_row().cells
            row_cells[0].text = metric
            row_cells[1].text = value
        
        # Thematic Code Structure
        doc.add_heading('Thematic Code Structure', level=1)
        
        if taxonomy.get('codes'):
            doc.add_paragraph('The following table presents the identified thematic codes:')
            
            codes_table = doc.add_table(rows=1, cols=4)
            codes_table.style = 'Table Grid'
            codes_hdr = codes_table.rows[0].cells
            codes_hdr[0].text = 'Code ID'
            codes_hdr[1].text = 'Name'
            codes_hdr[2].text = 'Description'
            codes_hdr[3].text = 'Level'
            
            for code in taxonomy.get('codes', [])[:20]:  # Limit to 20 codes for readability
                row_cells = codes_table.add_row().cells
                row_cells[0].text = code.get('id', '')
                row_cells[1].text = code.get('name', '')
                row_cells[2].text = code.get('description', '')[:100] + ('...' if len(code.get('description', '')) > 100 else '')
                row_cells[3].text = str(code.get('level', 0))
        
        # Quote Analysis
        doc.add_heading('Representative Quotes', level=1)
        doc.add_paragraph('The following section presents representative quotes for each major theme:')
        
        sample_count = 0
        for interview in interviews[:3]:  # Sample from first 3 interviews
            if sample_count >= 15:
                break
                
            doc.add_heading(f'Interview: {interview.get("interview_id", "Unknown")}', level=2)
            
            for quote in interview.get('quotes', [])[:5]:  # Sample 5 quotes per interview
                if sample_count >= 15:
                    break
                
                # Quote text
                quote_para = doc.add_paragraph()
                quote_para.add_run('"').bold = True
                quote_para.add_run(quote.get('text', '')[:300])
                if len(quote.get('text', '')) > 300:
                    quote_para.add_run('...')
                quote_para.add_run('"').bold = True
                
                # Speaker and codes
                speaker = quote.get('speaker', {}).get('name', 'Unknown')
                codes = ', '.join(quote.get('code_names', []))
                
                meta_para = doc.add_paragraph()
                meta_para.add_run(f'Speaker: {speaker}').italic = True
                meta_para.add_run(' | ')
                meta_para.add_run('Codes: ').bold = True
                meta_para.add_run(codes)
                
                sample_count += 1
        
        # Methodology
        doc.add_heading('Methodology', level=1)
        
        doc.add_heading('Data Collection', level=2)
        doc.add_paragraph(
            'Qualitative interviews were systematically analyzed using thematic coding '
            'approaches enhanced with LLM-assisted pattern recognition for comprehensive '
            'theme identification.'
        )
        
        doc.add_heading('Coding Process', level=2)
        doc.add_paragraph(
            'Thematic codes were developed through both inductive and deductive approaches, '
            'incorporating confidence scoring and validation mechanisms to ensure analytical rigor.'
        )
        
        doc.add_heading('Analysis Framework', level=2)
        doc.add_paragraph(
            'The analysis employed qualitative comparative analysis (QCA) principles for '
            'systematic pattern identification and cross-case comparison.'
        )
        
        # Conclusions
        doc.add_heading('Conclusions', level=1)
        doc.add_paragraph(
            'This comprehensive qualitative analysis reveals significant thematic patterns '
            'within the interview data. The systematic coding approach has identified key '
            'themes that provide valuable insights into the research domain. These findings '
            'establish a foundation for further theoretical development and empirical investigation.'
        )
        
        # Technical Appendix
        doc.add_heading('Technical Details', level=1)
        tech_details = [
            f'Analysis System: QCA Pipeline',
            f'Generation Date: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}',
            f'Export Format: Microsoft Word Report',
            f'Document Version: 1.0'
        ]
        
        for detail in tech_details:
            doc.add_paragraph(detail, style='List Bullet')
        
        # Save Word document
        doc.save(output_path)
        
        logger.info(f"Exported Word report: {output_path}")
        return str(output_path)
    
    def export_graphml_network(self, interviews: List[Dict[str, Any]], output_file: str = None) -> str:
        """
        Export code co-occurrence network as GraphML format for network analysis
        
        Args:
            interviews: List of interview data dictionaries
            output_file: Output filename (auto-generated if None)
            
        Returns:
            Path to generated GraphML file
        """
        if not NETWORKX_AVAILABLE:
            raise ImportError("networkx required for GraphML export")
        
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"qca_code_network_{timestamp}.graphml"
            
        output_path = self.output_dir / output_file
        
        # Create network graph
        G = nx.Graph()
        
        # Collect code co-occurrences
        code_counts = defaultdict(int)
        cooccurrence_counts = defaultdict(int)
        
        for interview in interviews:
            interview_id = interview.get('interview_id', 'Unknown')
            
            for quote in interview.get('quotes', []):
                codes = quote.get('code_names', [])
                
                # Count individual code frequencies
                for code in codes:
                    code_counts[code] += 1
                
                # Count co-occurrences (pairs of codes in same quote)
                for i, code1 in enumerate(codes):
                    for code2 in codes[i+1:]:
                        pair = tuple(sorted([code1, code2]))
                        cooccurrence_counts[pair] += 1
        
        # Add nodes (codes)
        for code, count in code_counts.items():
            G.add_node(code, 
                      frequency=count,
                      node_type='thematic_code',
                      label=code)
        
        # Add edges (co-occurrences)
        for (code1, code2), count in cooccurrence_counts.items():
            if count > 1:  # Only include co-occurrences that happen more than once
                G.add_edge(code1, code2, 
                          weight=count,
                          cooccurrence_frequency=count,
                          edge_type='cooccurrence')
        
        # Add network statistics as graph attributes
        G.graph['total_nodes'] = G.number_of_nodes()
        G.graph['total_edges'] = G.number_of_edges()
        G.graph['density'] = nx.density(G)
        G.graph['generated_date'] = datetime.now().isoformat()
        G.graph['analysis_type'] = 'thematic_code_cooccurrence'
        G.graph['description'] = 'Code co-occurrence network from qualitative coding analysis'
        
        # Calculate node centrality measures
        if G.number_of_nodes() > 1:
            centrality = nx.degree_centrality(G)
            betweenness = nx.betweenness_centrality(G)
            closeness = nx.closeness_centrality(G)
            
            # Add centrality measures as node attributes
            for node in G.nodes():
                G.nodes[node]['degree_centrality'] = centrality.get(node, 0)
                G.nodes[node]['betweenness_centrality'] = betweenness.get(node, 0)
                G.nodes[node]['closeness_centrality'] = closeness.get(node, 0)
        
        # Save GraphML file
        nx.write_graphml(G, output_path)
        
        logger.info(f"Exported GraphML network: {output_path}")
        logger.info(f"Network stats - Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}, Density: {nx.density(G):.3f}")
        return str(output_path)
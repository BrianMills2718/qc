"""
Code-First Architecture Schemas for 4-Phase Extraction Pipeline
"""

from pydantic import BaseModel, Field, field_validator
from typing import List, Optional, Dict, Any, Union
from enum import Enum


# ============================================================================
# Configuration Models
# ============================================================================

class ExtractionApproach(str, Enum):
    """Extraction approach for codes, speakers, or entities"""
    OPEN = "open"      # Discover from data
    CLOSED = "closed"  # Use predefined only
    MIXED = "mixed"    # Start with predefined, discover additional


class ExtractionConfig(BaseModel):
    """Main configuration for the extraction pipeline"""
    # Core configuration
    analytic_question: str = Field(..., description="Research question driving the analysis")
    interview_files_dir: str = Field(..., description="Directory containing interview files (DOCX or TXT)")
    
    # Code configuration
    coding_approach: ExtractionApproach = ExtractionApproach.OPEN
    predefined_codes_file: Optional[str] = None  # Path to DOCX/TXT with code definitions
    code_hierarchy_depth: int = 2  # Default hierarchy depth
    
    # Speaker configuration
    speaker_approach: ExtractionApproach = ExtractionApproach.OPEN
    predefined_speaker_schema_file: Optional[str] = None  # Path to DOCX/TXT with speaker properties
    
    # Entity/Relationship configuration
    entity_approach: ExtractionApproach = ExtractionApproach.OPEN
    predefined_entity_schema_file: Optional[str] = None  # Path to DOCX/TXT with entity/relationship types
    
    # Output configuration
    output_dir: str = "output"
    auto_import_neo4j: bool = True
    
    # Neo4j connection settings
    neo4j_uri: Optional[str] = None
    neo4j_user: Optional[str] = None
    neo4j_password: Optional[str] = None
    
    # LLM configuration
    llm_model: str = "gemini/gemini-2.5-flash"
    temperature: float = 0.1  # Low for consistency
    
    # Performance configuration
    max_concurrent_interviews: int = 5  # Maximum parallel interview processing
    
    # Analysis paradigm
    paradigm: str = "expert_qualitative_coder"  # Analysis lens for coding
    
    @property
    def interview_files(self) -> List[str]:
        """Get all interview files from the directory"""
        import os
        from pathlib import Path
        
        directory = Path(self.interview_files_dir)
        if not directory.exists():
            raise ValueError(f"Interview directory does not exist: {self.interview_files_dir}")
        
        # Find all DOCX and TXT files
        interview_files = []
        for ext in ['.docx', '.txt']:
            interview_files.extend(directory.glob(f"*{ext}"))
        
        # Convert to strings and sort for consistency
        return sorted([str(f) for f in interview_files])


# ============================================================================
# Phase 0: Schema Parsing Models (for closed/mixed approaches)
# ============================================================================

class ParsedCodeDefinition(BaseModel):
    """User-provided code definition parsed from text/docx"""
    name: str
    description: str
    parent_name: Optional[str] = None  # For hierarchical codes
    examples: List[str] = []  # Example quotes


class ParsedCodeSchema(BaseModel):
    """Complete parsed code taxonomy from user file"""
    codes: List[ParsedCodeDefinition]
    hierarchy_depth: int
    parsing_confidence: float


class ParsedSpeakerProperty(BaseModel):
    """Single speaker property definition"""
    name: str  # e.g., "role", "organization"
    property_type: str  # "string", "list", "number"
    required: bool = False
    allowed_values: Optional[List[str]] = None  # For categorical properties
    description: Optional[str] = None


class ParsedSpeakerSchema(BaseModel):
    """Complete parsed speaker schema from user file"""
    properties: List[ParsedSpeakerProperty]
    parsing_confidence: float


class ParsedEntityType(BaseModel):
    """Entity type definition"""
    name: str  # e.g., "Person", "Organization", "Technology"
    description: str
    identifying_patterns: List[str] = []  # Patterns that suggest this entity type


class ParsedRelationshipType(BaseModel):
    """Relationship type definition"""
    name: str  # e.g., "USES", "WORKS_FOR", "DEVELOPS"
    description: str
    valid_source_types: List[str] = []  # Entity types that can be source
    valid_target_types: List[str] = []  # Entity types that can be target
    directional: bool = True


class ParsedEntitySchema(BaseModel):
    """Complete parsed entity/relationship schema from user file"""
    entity_types: List[ParsedEntityType]
    relationship_types: List[ParsedRelationshipType]
    parsing_confidence: float


# ============================================================================
# Phase 1: Code Taxonomy Discovery Models
# ============================================================================

class HierarchicalCode(BaseModel):
    """Single code in the hierarchical taxonomy"""
    id: str
    name: str
    description: str
    semantic_definition: str  # LLM-generated detailed meaning
    parent_id: Optional[str] = None
    level: int  # 0 for root, 1 for first level, etc.
    example_quotes: List[str] = []  # Examples from discovery phase
    discovery_confidence: float = 0.8


class CodeTaxonomy(BaseModel):
    """Complete hierarchical code structure from Phase 1"""
    codes: List[HierarchicalCode]
    total_codes: int
    hierarchy_depth: int
    discovery_method: str  # "open", "closed", or "mixed"
    analytic_question: str
    extraction_confidence: float = 0.8


# ============================================================================
# Phase 2: Speaker Schema Discovery Models
# ============================================================================

class DiscoveredSpeakerProperty(BaseModel):
    """Speaker property discovered from interviews"""
    name: str
    description: Optional[str] = None  # Description of the property
    property_type: str  # "string", "list", "number", "categorical"
    frequency: int  # How often this property appears
    example_values: List[Union[str, int, float, bool]]  # Sample values found (can be mixed types)
    is_categorical: bool  # True if limited set of values
    possible_values: Optional[List[Union[str, int, float, bool]]] = None  # For categorical
    confidence: float = 0.8
    
    @field_validator('example_values', 'possible_values', mode='before')
    def convert_to_strings(cls, v):
        """Convert all values to strings for consistency"""
        if v is None:
            return v
        return [str(item) for item in v]


class SpeakerPropertySchema(BaseModel):
    """Complete speaker property schema from Phase 2"""
    properties: List[DiscoveredSpeakerProperty]
    # total_speakers_found removed - Phase 2 discovers properties, not speakers
    discovery_method: str  # "open", "closed", or "mixed"
    extraction_confidence: float = 0.8


# ============================================================================
# Phase 3: Entity/Relationship Schema Discovery Models
# ============================================================================

class DiscoveredEntityType(BaseModel):
    """Entity type discovered from interviews"""
    id: str  # Type ID in CAPS_SNAKE_CASE format (e.g., "AI_TOOL")
    name: str  # Human-readable name (e.g., "AI Tool")
    description: str
    frequency: int  # How often this type appears
    example_entities: List[str]  # Sample entities of this type
    common_contexts: List[str]  # Contexts where this type appears
    confidence: float = 0.8


class RelationshipExample(BaseModel):
    """Example relationship instance"""
    source: str
    target: str
    context: Optional[str] = None

class DiscoveredRelationshipType(BaseModel):
    """Relationship type discovered from interviews"""
    id: str  # Type ID in CAPS format (e.g., "USES", "MANAGES")
    name: str  # Human-readable name (e.g., "Uses", "Manages")
    description: str
    frequency: int
    common_source_types: List[str]  # Entity types commonly as source
    common_target_types: List[str]  # Entity types commonly as target
    directional: bool = True
    example_relationships: List[RelationshipExample]  # Examples with source/target
    confidence: float = 0.8


class EntityRelationshipSchema(BaseModel):
    """Complete entity/relationship schema from Phase 3"""
    entity_types: List[DiscoveredEntityType]
    relationship_types: List[DiscoveredRelationshipType]
    total_entities_found: int
    total_relationships_found: int
    discovery_method: str  # "open", "closed", or "mixed"
    extraction_confidence: float = 0.8


# ============================================================================
# Phase 4: Per-Interview Application Models with Dialogue Structure
# ============================================================================

class DialogueTurn(BaseModel):
    """Individual dialogue turn with conversational context"""
    turn_id: str  # Unique identifier for this turn
    sequence_number: int  # Order in the conversation (1, 2, 3...)
    speaker_name: str
    timestamp: Optional[str] = None  # If available (e.g., "0:03", "1:08")
    raw_location: str  # Original location marker (e.g., "Todd Helmus 0:03")
    turn_type: str = "statement"  # "statement", "question", "response", "interruption"
    responds_to_turn: Optional[str] = None  # ID of turn this responds to
    
    # Content structure
    text: str  # Complete turn text
    semantic_segments: List[str] = []  # Break down into codable segments
    
    # Discourse features
    contains_question: bool = False
    contains_response_markers: bool = False  # "yes", "right", "exactly"
    references_previous_speaker: bool = False  # "as John said", "building on that"
    
    # Metadata
    word_count: int = 0
    extraction_confidence: float = 0.8

class ConversationalContext(BaseModel):
    """Context about the conversational flow around a quote"""
    preceding_turns: List[str] = []  # IDs of 2-3 turns before this quote
    following_turns: List[str] = []  # IDs of 1-2 turns after this quote
    conversational_thread: Optional[str] = None  # Topic thread this quote belongs to
    turn_taking_pattern: str = "sequential"  # "sequential", "overlapping", "interrupted"
    
    # Response relationships
    is_response_to: Optional[str] = None  # Quote ID this responds to
    generates_responses: List[str] = []  # Quote IDs that respond to this
    
    # Topic flow
    topic_continuity: str = "continues"  # "initiates", "continues", "shifts", "returns"
    topic_markers: List[str] = []  # Words indicating topic flow


class ThematicConnection(BaseModel):
    """Thematic connection between two dialogue segments/quotes"""
    source_quote_id: str  # ID of the earlier quote
    target_quote_id: str  # ID of the later quote
    connection_type: str  # "addresses", "builds_on", "relates_to", "contradicts", "exemplifies"
    confidence_score: float  # 0.0-1.0 confidence in this connection
    evidence: str  # Specific words/phrases that demonstrate the connection
    reasoning: str  # Detailed explanation of why this connection exists
    thematic_overlap: List[str]  # Shared concepts between the segments
    
    # Source information
    source_speaker: str
    source_position: int
    source_content: str
    
    # Target information
    target_speaker: str
    target_position: int
    target_content: str
    
    # Analysis metadata
    detection_timestamp: Optional[str] = None
    analysis_confidence: float = 0.8


class SpeakerInfo(BaseModel):
    """Enhanced speaker information with discourse patterns"""
    name: str  # Best guess at speaker name/identifier
    confidence: float  # Confidence in identification (0-1)
    identification_method: str  # How speaker was identified
    quotes_count: int = 0  # Number of quotes from this speaker
    
    # Discourse patterns
    typical_turn_length: int = 0  # Average words per turn
    interaction_frequency: float = 0.0  # Turns per total turns
    response_patterns: List[str] = []  # Common ways this speaker responds
    topic_initiation_count: int = 0  # How often they start new topics


class ExtractedEntity(BaseModel):
    """Entity extracted from quote/interview"""
    name: str
    type: str  # From entity schema
    confidence: float = 0.8
    first_mention_quote_id: Optional[str] = None
    mention_count: int = 1
    contexts: List[str] = []  # Different contexts where mentioned


class ExtractedRelationship(BaseModel):
    """Relationship extracted from quote/interview"""
    source_entity: str
    source_type: Optional[str] = None  # Made optional - will be inferred if missing
    target_entity: str
    target_type: Optional[str] = None  # Made optional - will be inferred if missing
    relationship_type: str
    confidence: float = 0.8
    supporting_quote_ids: List[str] = []
    contexts: List[str] = []


class EnhancedQuote(BaseModel):
    """Quote with all associations and dialogue context"""
    id: str
    text: str  # Exact quote text
    context_summary: str  # Variable length context explanation
    
    # Code associations (many-to-many)
    code_ids: List[str]
    code_names: List[str]  # For readability
    code_confidence_scores: List[float]  # Per-code confidence
    
    # Speaker information (embedded)
    speaker: SpeakerInfo
    
    # Dialogue structure (NEW)
    dialogue_turn: DialogueTurn  # Complete turn information
    conversational_context: ConversationalContext  # Flow context
    
    # Provenance (enhanced)
    interview_id: str
    interview_title: str
    line_start: Optional[int] = None
    line_end: Optional[int] = None
    sequence_position: int  # Position in conversation flow (1, 2, 3...)
    
    # Entity/Relationship extraction at quote level
    quote_entities: List[ExtractedEntity]
    quote_relationships: List[ExtractedRelationship]
    
    # Overall confidence
    extraction_confidence: float = 0.8


class CodedInterview(BaseModel):
    """Complete coded interview from Phase 4"""
    interview_id: str
    interview_title: str
    interview_file: str
    
    # Extracted quotes mapped to codes
    quotes: List[EnhancedQuote]
    
    # All speakers found in interview
    speakers: List[SpeakerInfo]
    
    # Interview-level entities/relationships
    interview_entities: List[ExtractedEntity]
    interview_relationships: List[ExtractedRelationship]
    
    # Dialogue structure (NEW)
    dialogue_turns: List[DialogueTurn] = []  # All dialogue turns in the interview
    thematic_connections: List[ThematicConnection] = []  # Cross-speaker thematic links
    
    # Statistics
    total_quotes: int
    total_codes_applied: int
    unique_codes_used: List[str]
    total_speakers: int
    total_entities: int
    total_relationships: int
    
    # Metadata
    extraction_timestamp: str
    extraction_confidence: float = 0.8
    processing_time_seconds: float


# ============================================================================
# Phase 4A: Simplified Quote and Speaker Extraction Models
# ============================================================================

class SimpleQuote(BaseModel):
    """Simplified quote for Phase 4A extraction"""
    text: str  # Exact quote text
    speaker_name: str  # Speaker identifier
    code_ids: List[str]  # Code IDs from taxonomy (e.g., ["AI_IMPACT_RESEARCH_TASKS"])
    location_start: Optional[int] = None  # Line or paragraph number
    location_end: Optional[int] = None    # Line or paragraph end (if multi-line/para)
    location_type: Optional[str] = None   # "line" or "paragraph"

class QuotesAndSpeakers(BaseModel):
    """Phase 4A: Extract quotes and speakers only"""
    quotes: List[SimpleQuote]
    speakers: List[SpeakerInfo]
    total_quotes: int
    total_codes_applied: int

# ============================================================================
# Phase 4B: Entity and Relationship Extraction Models
# ============================================================================

class EntitiesAndRelationships(BaseModel):
    """Phase 4B: Extract entities and relationships only"""
    entities: List[ExtractedEntity]
    relationships: List[ExtractedRelationship]
    total_entities: int
    total_relationships: int

# ============================================================================
# Global Aggregation Models (Post-Phase 4)
# ============================================================================

class GlobalEntity(BaseModel):
    """Entity aggregated across all interviews"""
    name: str
    type: str
    total_mentions: int
    interview_ids: List[str]  # Interviews where mentioned
    quote_ids: List[str]  # All quotes mentioning this entity
    related_codes: List[str]  # Codes associated with quotes mentioning this
    confidence: float = 0.8


class GlobalRelationship(BaseModel):
    """Relationship aggregated across all interviews"""
    source_entity: str
    target_entity: str
    relationship_type: str
    total_mentions: int
    interview_ids: List[str]
    quote_ids: List[str]
    related_codes: List[str]
    confidence: float = 0.8


class ExtractionResults(BaseModel):
    """Complete extraction results across all phases"""
    # Configuration used
    config: ExtractionConfig
    
    # Phase 1 results
    code_taxonomy: CodeTaxonomy
    
    # Phase 2 results
    speaker_schema: SpeakerPropertySchema
    
    # Phase 3 results
    entity_relationship_schema: EntityRelationshipSchema
    
    # Phase 4 results (per interview)
    coded_interviews: List[CodedInterview]
    
    # Global aggregations
    global_entities: List[GlobalEntity]
    global_relationships: List[GlobalRelationship]
    
    # Overall statistics
    total_interviews_processed: int
    total_quotes_extracted: int
    total_unique_speakers: int
    total_unique_entities: int
    total_unique_relationships: int
    
    # Processing metadata
    extraction_timestamp: str
    total_processing_time_seconds: float
    llm_tokens_used: int
    overall_confidence: float = 0.8


# ============================================================================
# Utility Models for Neo4j Import
# ============================================================================

class Neo4jImportData(BaseModel):
    """Data formatted for Neo4j import"""
    # Node types
    codes: List[Dict[str, Any]]  # Code nodes with hierarchy
    quotes: List[Dict[str, Any]]  # Quote nodes
    speakers: List[Dict[str, Any]]  # Speaker nodes
    entities: List[Dict[str, Any]]  # Entity nodes
    interviews: List[Dict[str, Any]]  # Interview nodes
    
    # Relationship types
    code_hierarchies: List[Dict[str, Any]]  # PARENT_OF relationships
    quote_to_codes: List[Dict[str, Any]]  # SUPPORTS relationships
    quote_to_speakers: List[Dict[str, Any]]  # SPOKEN_BY relationships
    quote_to_entities: List[Dict[str, Any]]  # MENTIONS relationships
    entity_relationships: List[Dict[str, Any]]  # Domain relationships
    quote_to_interviews: List[Dict[str, Any]]  # FROM_INTERVIEW relationships
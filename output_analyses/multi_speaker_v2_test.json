{
  "completed_at": "2026-02-11T14:34:45.967355",
  "config": {},
  "created_at": "2026-02-11T14:29:55.566870",
  "id": "job_20260211_142955",
  "interviews_count": 3,
  "results": {
    "analysis_summary": "Analyzed 3 interview(s) using structured qualitative coding",
    "codes_identified": [
      {
        "code": "Methods Diversity and Common Applications",
        "confidence": 0.9,
        "mention_count": 3
      },
      {
        "code": "AI Opportunities for Research Methods",
        "confidence": 0.9,
        "mention_count": 3
      },
      {
        "code": "Document Parsing & Automatic Network Extraction",
        "confidence": 0.7,
        "mention_count": 2
      },
      {
        "code": "Interview and Qualitative Coding Automation",
        "confidence": 0.8,
        "mention_count": 3
      },
      {
        "code": "Literature Review and Search Support",
        "confidence": 0.85,
        "mention_count": 3
      },
      {
        "code": "Data Collection, Coding, and Quality Challenges",
        "confidence": 0.9,
        "mention_count": 3
      },
      {
        "code": "CUI Handling and Domain-Specific Language Barriers",
        "confidence": 0.8,
        "mention_count": 3
      },
      {
        "code": "Tools, Software Practices, and Workflow Automation",
        "confidence": 0.9,
        "mention_count": 3
      },
      {
        "code": "R/Python, Deduce, Excel and Coding Practices",
        "confidence": 0.8,
        "mention_count": 3
      },
      {
        "code": "Community Fragmentation, Discovery, and Knowledge Sharing",
        "confidence": 0.85,
        "mention_count": 3
      },
      {
        "code": "Need for Central Repositories and People-Finder Tools",
        "confidence": 0.8,
        "mention_count": 3
      },
      {
        "code": "Adoption Barriers, Use Cases, and Implementation Strategy",
        "confidence": 0.8,
        "mention_count": 3
      },
      {
        "code": "Risks, Hallucination, De-identification, and Trust",
        "confidence": 0.9,
        "mention_count": 3
      },
      {
        "code": "Social Network Analysis: Use Cases, Workflow, and Scale",
        "confidence": 0.75,
        "mention_count": 1
      }
    ],
    "data_warnings": [
      "Interview 'RAND AI and Research Methods Discussion.ISDP.docx' appears truncated (ends mid-sentence)",
      "Interview 'RAND Methods and AI. Resource Management, PAF.docx' appears truncated (ends mid-sentence)"
    ],
    "full_analysis": "=== COMPREHENSIVE QUALITATIVE CODING ANALYSIS ===\n\nPHASE 1: HIERARCHICAL CODE DISCOVERY\n{\n  \"codes\": [\n    {\n      \"id\": \"METHODS_AND_APPLICATIONS\",\n      \"name\": \"Methods Diversity and Common Applications\",\n      \"description\": \"Describes the wide range of research methods used across RAND and within units (qualitative, quantitative, modeling, cost analysis, network analysis) and the common application contexts (literature reviews, interviews, wargames, desk research, bespoke models). Captures statements about frequency, breadth, and role of methods in shaping project scoping and outcomes. Emphasizes that many projects combine methods and that method choice is often client-driven.\",\n      \"semantic_definition\": \"Any interviewee statement describing types of research methods used, their frequency, typical application areas, or how methods are combined to answer problems (e.g., literature reviews, interviews, cost analysis, modeling, network analysis).\",\n      \"parent_id\": null,\n      \"level\": 0,\n      \"example_quotes\": [\n        \"A lot of the applications of network analysis are embedded in research not involved in network analysis.\",\n        \"Yeah, I mean, I would think a lot of literature reviews, most of our work is heavily qualitative.\",\n        \"Our unit is quite diverse... we use almost all the methods.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.9\n    },\n    {\n      \"id\": \"AI_OPPORTUNITIES\",\n      \"name\": \"AI Opportunities for Research Methods\",\n      \"description\": \"Statements about where AI (LLMs, automation tools) could add value to research methods — e.g., automating routine steps, extracting structured data from text, assisting literature reviews, augmenting wargaming, and producing dissemination products. Includes discussion of what is already semi-automatable and what remains exploratory. Emphasizes potential for lowering barriers and scaling methods.\",\n      \"semantic_definition\": \"Any interviewee comment proposing, describing, or evaluating potential uses of AI to support research workflows, analysis steps, data transformation, or dissemination (excluding purely technical tool preference statements which belong in Tools).\",\n      \"parent_id\": null,\n      \"level\": 0,\n      \"example_quotes\": [\n        \"That is a step that emerging AI and LLM is making possible and doable. This is the new thing that AI allows.\",\n        \"I mean, I still think like we spent a lot of time and energy on projects... And that is an area where AI could be well applied.\",\n        \"Internally it would be useful to be able to identify which RAND products are using network analysis and who is doing it.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.9\n    },\n    {\n      \"id\": \"DOCUMENT_PARSING_TO_NETWORK\",\n      \"name\": \"Document Parsing & Automatic Network Extraction\",\n      \"description\": \"Discussion of converting formal documents and doctrine into structured relational/network data by identifying actors, interactions, and flows. Covers examples where a researcher manually coded doctrine into network data and the prospect of automating that extraction with AI. Notes that automation needs clear signal definitions to code reliably.\",\n      \"semantic_definition\": \"Statements that specifically describe or illustrate extracting actors/relations from textual documents (doctrine, procedures, manuals) to create network data or to automate that conversion.\",\n      \"parent_id\": \"AI_OPPORTUNITIES\",\n      \"level\": 1,\n      \"example_quotes\": [\n        \"He used a 3-30 document that described the procedure for the air operations center... The first step is figuring out who all the actors are, who are the ppl, what data are they passing and he did it just entirely by looking at what was written in doctrine. The first step from text documents which presumably you could automate.\",\n        \"Reviewing materials provided by the military... the language used by the military can be different.\"\n      ],\n      \"mention_count\": 2,\n      \"discovery_confidence\": 0.7\n    },\n    {\n      \"id\": \"INTERVIEW_CODING_AUTOMATION\",\n      \"name\": \"Interview and Qualitative Coding Automation\",\n      \"description\": \"Interest in automating or augmenting coding of interview transcripts and qualitative notes to speed analysis (e.g., using deduce, Muse, or LLM-based tools). Interviewees described both the desire for internal coding tools and the current ad hoc practices (Excel, deduce) and raised questions about validity and workflow integration.\",\n      \"semantic_definition\": \"Any mention of automating, tooling for, or improving the coding and synthesis of interview, focus group, or other qualitative textual data.\",\n      \"parent_id\": \"AI_OPPORTUNITIES\",\n      \"level\": 1,\n      \"example_quotes\": [\n        \"coding would be really helpful... For just coding large bodies of interview notes.\",\n        \"There are a lot of different people... putting together the training tools and lectures so people interested have a place to go.\",\n        \"Like even if you start with like a human driven literature review, maybe you then have the AI like trying to figure out what you might have missed.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.8\n    },\n    {\n      \"id\": \"LIT_REVIEW_AND_SEARCH_AUTOMATION\",\n      \"name\": \"Literature Review and Search Support\",\n      \"description\": \"Use cases where AI could accelerate or improve literature and doctrinal reviews, search across unstructured repositories, and surface missed resources. Interviewees described both opportunities (speed, breadth, identifying missed sources) and limitations (classification barriers, quality of results).\",\n      \"semantic_definition\": \"Statements about using AI/LLMs to find, summarize, scan, or augment literature and doctrine reviews, including searching constrained/unclassified/classified corpora and surfacing relevant materials.\",\n      \"parent_id\": \"AI_OPPORTUNITIES\",\n      \"level\": 1,\n      \"example_quotes\": [\n        \"We ran LLM on five years worth of reports to look at methods.\",\n        \"a lot of literature reviews, most of our work is heavily qualitative... that is an area where AI could be well applied.\",\n        \"Can AI help with class? I don't know... I think it's probably the difficulty times the value is probably still high.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.85\n    },\n    {\n      \"id\": \"DATA_COLLECTION_AND_QUALITY\",\n      \"name\": \"Data Collection, Coding, and Quality Challenges\",\n      \"description\": \"Challenges around getting suitable data (survey response, structured relational data), coding textual sources into structured formats, and maintaining reproducible expertise. Interviewees emphasized that collecting good data is often the limiting step and that many projects use ad hoc approaches.\",\n      \"semantic_definition\": \"Any statement about practical difficulties in acquiring, coding, validating, or preparing data for analysis, including remarks about survey response, bespoke coding, and the limits of institutionalizing expertise.\",\n      \"parent_id\": null,\n      \"level\": 0,\n      \"example_quotes\": [\n        \"Ultimately once you commit to doing network analysis the biggest issue is collecting and coding the data.\",\n        \"The first method described above is automatable because it involves sending out emails and getting responses in structured format.\",\n        \"If they don't get the data and code it properly... RAND does not have an ability to depersonalize that expertise.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.9\n    },\n    {\n      \"id\": \"CUI_AND_DOMAIN_SPECIFIC_LANGUAGE\",\n      \"name\": \"CUI Handling and Domain-Specific Language Barriers\",\n      \"description\": \"Concerns about handling Controlled Unclassified Information (CUI), domain-specific jargon (military doctrine, TTPS), and the need for tools trained on service-specific language. Interviewees stressed that many documents are CUI and that general-purpose models struggle with specialized military language.\",\n      \"semantic_definition\": \"Statements about data sensitivity (CUI), the prevalence of protected or service-specific documents, and the need for models/tools that respect CUI and understand domain-specific terminology.\",\n      \"parent_id\": \"DATA_COLLECTION_AND_QUALITY\",\n      \"level\": 1,\n      \"example_quotes\": [\n        \"At least half of the time, at least one of those documents that we get will be cui.\",\n        \"The language used by the military can be different. It is its own thing, so it can be a little bit hard to do that with some of the off the shelf AI tools.\",\n        \"De-identifying data and… harder to do if AI might be able to go through and unravel the signals and identities.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.8\n    },\n    {\n      \"id\": \"TOOLS_AND_WORKFLOWS\",\n      \"name\": \"Tools, Software Practices, and Workflow Automation\",\n      \"description\": \"Descriptions of analysis tools and common software stacks (R, Python, Gephi, deduce, Excel) and discussion of automating repetitive workflow steps (formatting, templates, citation management, graphical outputs). Interviewees noted heterogeneity in choices and interest in standardized snippets/libraries or internal services.\",\n      \"semantic_definition\": \"Any interviewee statements naming specific tools, languages, software habits, or describing repeatable workflow steps that are candidates for automation (e.g., R/Python workflows, visualization, template formatting).\",\n      \"parent_id\": null,\n      \"level\": 0,\n      \"example_quotes\": [\n        \"Some visualization that is done in Geffe, a lot of the heavy duty analysis, 70 percent in R and 30 percent in python.\",\n        \"deduce I think is the most commonly used tool. Broadly at RAND.\",\n        \"If you had a standard set of R libraries, where first cut in graphical representation in publication quality, that is snippets of code you can use.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.9\n    },\n    {\n      \"id\": \"R_PYTHON_AND_SOFTWARE_PRACTICES\",\n      \"name\": \"R/Python, Deduce, Excel and Coding Practices\",\n      \"description\": \"Specifics about programming and qualitative tools in use: the prevalence of R and Python for heavy analysis, Gephi for visualization, deduce or Excel-based workflows for qualitative coding, and the idea of sharing reusable code snippets or templates. Highlights heterogeneity of practices and license/access constraints.\",\n      \"semantic_definition\": \"Statements naming R, Python, Gephi, deduce, Excel, or other software practices and describing how researchers use them or the barriers (licenses, variability) to standardization.\",\n      \"parent_id\": \"TOOLS_AND_WORKFLOWS\",\n      \"level\": 1,\n      \"example_quotes\": [\n        \"a lot of the heavy duty analysis, 70 percent in R and 30 percent in python.\",\n        \"a lot of people do coding in Excel. I've seen a lot of like very effective ways of coding in Excel.\",\n        \"deduce I think is the most commonly used tool. Broadly at RAND.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.8\n    },\n    {\n      \"id\": \"COMMUNITY_AND_ORGANIZATIONAL_ISSUES\",\n      \"name\": \"Community Fragmentation, Discovery, and Knowledge Sharing\",\n      \"description\": \"Interviewees described a fragmented community and difficulty discovering who is doing what (multiple intranet pages, scattered tools, inconsistent inventories). They expressed a desire to identify practitioners, build communities of practice, and centralize knowledge to enable cross-pollination and reuse.\",\n      \"semantic_definition\": \"Any reference to fragmentation of expertise, difficulty finding people or past products, multiple conflicting internal resources, or the need for directories, communities of practice, or centralized repositories.\",\n      \"parent_id\": null,\n      \"level\": 0,\n      \"example_quotes\": [\n        \"There is a very diverse set of people that use the method across RAND and people are not always in conversation with each other, calling it different things and not always in conversation with each other.\",\n        \"there's two different pages on the RAND Intranet site that tell you what RAND's AI resources are like.\",\n        \"I sometimes go to other people's project folders and I'm like, oh, they have... I sometimes go to other people's project folders and I'm like, oh, they have.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.85\n    },\n    {\n      \"id\": \"KNOWLEDGE_SHARING_REPOSITORY_NEEDS\",\n      \"name\": \"Need for Central Repositories and People-Finder Tools\",\n      \"description\": \"Proposals to create searchable catalogs or AI-backed people/finders that ingest reports, projects, and products to reveal who used which methods and what outputs exist. Interviewees emphasized time savings from having integrated metadata (sponsors, methods, deliverables) and the possibility of automated discovery of prior work relevant to new contracts.\",\n      \"semantic_definition\": \"Statements proposing or supporting a central index, searchable database, or AI tool that connects projects, publications, methods, and researchers to improve discoverability and reuse.\",\n      \"parent_id\": \"COMMUNITY_AND_ORGANIZATIONAL_ISSUES\",\n      \"level\": 1,\n      \"example_quotes\": [\n        \"If we could put all the ISDP reports into an AI and the AI tells us what methods we use. Since we talk about the methods in the reports like and have it pull out what methods we use.\",\n        \"There is a people Finder or whatever we call that has actually read everybody's research and incorporates the kinds of work they actually do.\",\n        \"Internally it would be useful to be able to identify which RAND products are using network analysis and who is doing it.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.8\n    },\n    {\n      \"id\": \"ADOPTION_BARRIERS_AND_STRATEGIES\",\n      \"name\": \"Adoption Barriers, Use Cases, and Implementation Strategy\",\n      \"description\": \"Discussion of barriers to adoption (lack of clear useful tools, disaggregation, sponsor appetite, training needs) and suggestions for pragmatic strategies (start with low-hanging fruit like formatting, templates, reproducible snippets, and build trust with functional tools). Emphasizes real-world incentives and the 'if it works people will use it' approach.\",\n      \"semantic_definition\": \"Statements about what hinders or encourages adoption of AI/tools (organizational friction, missing capabilities, sponsor expectations) and recommended practical strategies to increase uptake across researchers.\",\n      \"parent_id\": null,\n      \"level\": 0,\n      \"example_quotes\": [\n        \"The balancing act is figuring out what you want to show to the sponsor. Only sometimes is there an appetite for seeing hard core analysis moving beyond the pictures.\",\n        \"If you build it, they'll come.\",\n        \"I think if we actually have tools that are highly functional and truly deliver. People are gonna catch on pretty quickly.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.8\n    },\n    {\n      \"id\": \"RISKS_ETHICS_AND_VALIDITY\",\n      \"name\": \"Risks, Hallucination, De-identification, and Trust\",\n      \"description\": \"Concerns about AI hallucination, validity of automated outputs, data de-identification and confidentiality risks, and the limits of AI in producing original insight. Interviewees also emphasized the need for QA checks and processes to avoid incorrect citations or fabricated outputs.\",\n      \"semantic_definition\": \"Any interviewee remarks about potential harms, accuracy problems, confidentiality or de-identification risks, trustworthiness of AI outputs, hallucination, and the need for validation/QA processes.\",\n      \"parent_id\": null,\n      \"level\": 0,\n      \"example_quotes\": [\n        \"Hallucination.\",\n        \"I just don't think... I don't think it's trustworthy and I don't think it's capable of original thought.\",\n        \"ensuring the data is robust and valid and things are not getting miscoded and caste.\"\n      ],\n      \"mention_count\": 3,\n      \"discovery_confidence\": 0.9\n    },\n    {\n      \"id\": \"SOCIAL_NETWORK_ANALYSIS_SPECIFICS\",\n      \"name\": \"Social Network Analysis: Use Cases, Workflow, and Scale\",\n      \"description\": \"Detailed discussion of social network analysis workflows, common use cases (organization mapping, process-to-network conversion, embedded applications), tooling practices, and estimates of community size and project frequency at RAND. Includes examples of manual coding of doctrine into networks and the prospect of automating survey-based organizational mapping.\",\n      \"semantic_definition\": \"Statements that specifically address social network analysis methods, examples of applications, procedural descriptions (e.g., survey batteries, matrix population, mapping actor ties), and organizational estimates (community size, project counts).\",\n      \"parent_id\": null,\n      \"level\": 0,\n      \"example_quotes\": [\n        \"You can ask everyone questions, who do you go to for advice, who do you work with on a daily basis?... What happens is you map the organization and show it naturally segments into different groups and you can see where the fault lines are.\",\n        \"The second method... is taking the process or system and converting into a structured network. Jon Fujimora took process of the air ops center... and he coded it all by hand into network data.\",\n        \"About 100.\"\n      ],\n      \"mention_count\": 1,\n      \"discovery_confidence\": 0.75\n    }\n  ],\n  \"total_codes\": 13,\n  \"analysis_confidence\": 0.85\n}\n\nPHASE 2: MULTI-SPEAKER PERSPECTIVE ANALYSIS\n{\n  \"participants\": [\n    {\n      \"name\": \"Matthew Sergant\",\n      \"role\": \"RAND Research Methods Center — Social Network Analysis\",\n      \"characteristics\": [\n        \"Methods-center leader with deep practical experience in social network analysis\",\n        \"Focuses on operationalizing network methods across projects and building a practitioner community\",\n        \"Pragmatic about what is automatable vs. what requires human expertise\",\n        \"Familiar with tooling (R, Python, Gephi) and typical workflows\",\n        \"Attuned to data collection, coding bottlenecks, and sponsor expectations\"\n      ],\n      \"perspective_summary\": \"Sees social network analysis as widely embedded across RAND with recurring barriers in data collection/coding and knowledge sharing. Believes AI can meaningfully automate routinized steps (survey workflows, document-to-network extraction) if signal definitions are clear, but adoption depends on demonstrable, trustworthy tools and realistic sponsor expectations.\",\n      \"codes_emphasized\": [\n        \"SOCIAL_NETWORK_ANALYSIS_SPECIFICS\",\n        \"METHODS_AND_APPLICATIONS\",\n        \"DOCUMENT_PARSING_TO_NETWORK\",\n        \"DATA_COLLECTION_AND_QUALITY\",\n        \"TOOLS_AND_WORKFLOWS\",\n        \"COMMUNITY_AND_ORGANIZATIONAL_ISSUES\",\n        \"RISKS_ETHICS_AND_VALIDITY\"\n      ]\n    },\n    {\n      \"name\": \"Heather Williams\",\n      \"role\": \"ISDP Researcher (wargaming and qualitative methods)\",\n      \"characteristics\": [\n        \"Practitioner working on wargaming, structured discussion methods and qualitative synthesis\",\n        \"Interested in AI for game design, adjudication, simulation augmentation and literature synthesis\",\n        \"Concerned about AI trustworthiness and protecting original analytic insight\",\n        \"Aware of fragmentation of internal AI resources and duplication of effort\"\n      ],\n      \"perspective_summary\": \"Values AI for augmenting wargaming (adjudication, player simulation), literature review augmentation, and dissemination (e.g., podcast generation), but is cautious about over-reliance — questions trust, originality, and organizational fragmentation of tools and resources.\",\n      \"codes_emphasized\": [\n        \"AI_OPPORTUNITIES\",\n        \"LIT_REVIEW_AND_SEARCH_AUTOMATION\",\n        \"COMMUNITY_AND_ORGANIZATIONAL_ISSUES\",\n        \"TOOLS_AND_WORKFLOWS\",\n        \"ADOPTION_BARRIERS_AND_STRATEGIES\",\n        \"RISKS_ETHICS_AND_VALIDITY\",\n        \"METHODS_AND_APPLICATIONS\"\n      ]\n    },\n    {\n      \"name\": \"Lisa Saum-Manning\",\n      \"role\": \"ISDP Researcher (qualitative methods & dissemination)\",\n      \"characteristics\": [\n        \"Frequent user of literature reviews and qualitative interviews\",\n        \"Practical focus on coding workflows, dissemination, and QA\",\n        \"Open to internal AI tooling for routine tasks but worried about hallucination and validation\",\n        \"Interested in bringing cross-domain perspectives into recommendations\"\n      ],\n      \"perspective_summary\": \"Strongly supports AI to speed literature reviews, automate interview coding and generate dissemination products; emphasizes the need for QA, validation, and guardrails to avoid hallucination and maintain trust in outputs. Wants AI to broaden perspective (business literature, outside lessons) to improve actionable recommendations.\",\n      \"codes_emphasized\": [\n        \"LIT_REVIEW_AND_SEARCH_AUTOMATION\",\n        \"INTERVIEW_CODING_AUTOMATION\",\n        \"TOOLS_AND_WORKFLOWS\",\n        \"KNOWLEDGE_SHARING_REPOSITORY_NEEDS\",\n        \"ADOPTION_BARRIERS_AND_STRATEGIES\",\n        \"RISKS_ETHICS_AND_VALIDITY\"\n      ]\n    },\n    {\n      \"name\": \"Christopher Mouton\",\n      \"role\": \"ISDP Senior Researcher / Methods advocate\",\n      \"characteristics\": [\n        \"Experienced researcher focused on pragmatic adoption strategies\",\n        \"Advocates for incremental, high-value automation (formatting, templates, people-finder) as rapid wins\",\n        \"Concerned about fragmented discovery and poor documentation of methods across RAND\",\n        \"Wary of unvetted outputs being copy-pasted into official work\"\n      ],\n      \"perspective_summary\": \"Argues for prioritizing high-leverage, low-risk AI applications (document formatting, people-finder, reproducible snippets) that reduce friction and demonstrate value quickly. Sees community fragmentation and discovery problems as major barriers and warns about hallucination risk if tools are not well-governed.\",\n      \"codes_emphasized\": [\n        \"ADOPTION_BARRIERS_AND_STRATEGIES\",\n        \"COMMUNITY_AND_ORGANIZATIONAL_ISSUES\",\n        \"KNOWLEDGE_SHARING_REPOSITORY_NEEDS\",\n        \"TOOLS_AND_WORKFLOWS\",\n        \"RISKS_ETHICS_AND_VALIDITY\",\n        \"METHODS_AND_APPLICATIONS\"\n      ]\n    },\n    {\n      \"name\": \"Anna Jean Wirth\",\n      \"role\": \"Resource Management Program, Project Air Force (Program Lead / Researcher)\",\n      \"characteristics\": [\n        \"Leads a diverse portfolio (cost analysis, resilience, logistics, modeling)\",\n        \"Regularly conducts doctrine/policy document review and synthesizes parameters for models\",\n        \"Faces heavy CUI and service-specific language constraints\",\n        \"Uses heterogeneous toolset (deduce, Excel) and emphasizes reproducible processes\"\n      ],\n      \"perspective_summary\": \"Describes wide-ranging methods in her program and highlights that most projects require qualitative engagement to scope problems. Emphasizes practical barriers: military/domain-specific language, frequent CUI, and variability of documents. Recommends investment in CUI-ready, service-specific AI tools for document parsing and search to accelerate research.\",\n      \"codes_emphasized\": [\n        \"METHODS_AND_APPLICATIONS\",\n        \"DATA_COLLECTION_AND_QUALITY\",\n        \"CUI_AND_DOMAIN_SPECIFIC_LANGUAGE\",\n        \"LIT_REVIEW_AND_SEARCH_AUTOMATION\",\n        \"TOOLS_AND_WORKFLOWS\",\n        \"ADOPTION_BARRIERS_AND_STRATEGIES\"\n      ]\n    }\n  ],\n  \"consensus_themes\": [\n    \"AI offers clear value for routine, time-consuming research tasks (literature reviews, document search, formatting, transcription/summaries, and initial coding).\",\n    \"Researchers face fragmentation in tools, discovery, and knowledge about who is doing what — a centralized people-finder / repository would save time and increase reuse.\",\n    \"Qualitative work and speaking with subject-matter experts remain foundational across projects; AI should augment, not replace, domain elicitation and interpretation.\",\n    \"Adoption should prioritize pragmatic, low-risk wins (templates, formatting, reproducible snippets, people-finder) to build trust before tackling complex automated analysis.\",\n    \"There are real risks from AI (hallucination, validity, de-identification and CUI handling) that require QA, validation processes, and careful boundary-setting.\",\n    \"Many methods are heterogeneous across RAND; one-size-fits-all automation is unlikely — domain- and task-specific models/tools will be more useful.\"\n  ],\n  \"divergent_viewpoints\": [\n    \"Degree of trust and scope for AI: some participants (e.g., Christopher, Matthew) emphasize pragmatic uses and are optimistic about automating structured steps; others (e.g., Heather, Lisa) are more skeptical about AI's ability to produce original, trustworthy analytic insight.\",\n    \"Off-the-shelf vs. domain-specific models: some believe general LLMs can be useful for many tasks, while others (notably Anna Jean) insist on service-specific/CUI-capable tools because military language and protected documents defeat generic models.\",\n    \"Prioritization of investments: disagreement about whether to prioritize 'big' method automation (e.g., automating coding and analytic insight) or bite-sized productivity tools (formatting, templates, people-finder) as the first focus.\",\n    \"Perceived growth/role of methods like wargaming: Heather views gaming as steady and core; Christopher argues its renewed relevance given abstract, strategic questions — nuance in whether it is growing vs. enduring.\",\n    \"Comfort with internal vs. external AI platforms: some participants see RAND-specific tools (RandChat) as necessary for proprietary/CUI work, while others find external providers deliver better general-purpose results and expect mixed use.\"\n  ],\n  \"perspective_mapping\": {\n    \"Matthew Sergant\": [\n      \"SOCIAL_NETWORK_ANALYSIS_SPECIFICS\",\n      \"METHODS_AND_APPLICATIONS\",\n      \"DOCUMENT_PARSING_TO_NETWORK\",\n      \"DATA_COLLECTION_AND_QUALITY\",\n      \"TOOLS_AND_WORKFLOWS\",\n      \"COMMUNITY_AND_ORGANIZATIONAL_ISSUES\",\n      \"RISKS_ETHICS_AND_VALIDITY\"\n    ],\n    \"Heather Williams\": [\n      \"AI_OPPORTUNITIES\",\n      \"LIT_REVIEW_AND_SEARCH_AUTOMATION\",\n      \"COMMUNITY_AND_ORGANIZATIONAL_ISSUES\",\n      \"TOOLS_AND_WORKFLOWS\",\n      \"ADOPTION_BARRIERS_AND_STRATEGIES\",\n      \"RISKS_ETHICS_AND_VALIDITY\",\n      \"METHODS_AND_APPLICATIONS\"\n    ],\n    \"Lisa Saum-Manning\": [\n      \"LIT_REVIEW_AND_SEARCH_AUTOMATION\",\n      \"INTERVIEW_CODING_AUTOMATION\",\n      \"TOOLS_AND_WORKFLOWS\",\n      \"KNOWLEDGE_SHARING_REPOSITORY_NEEDS\",\n      \"ADOPTION_BARRIERS_AND_STRATEGIES\",\n      \"RISKS_ETHICS_AND_VALIDITY\"\n    ],\n    \"Christopher Mouton\": [\n      \"ADOPTION_BARRIERS_AND_STRATEGIES\",\n      \"COMMUNITY_AND_ORGANIZATIONAL_ISSUES\",\n      \"KNOWLEDGE_SHARING_REPOSITORY_NEEDS\",\n      \"TOOLS_AND_WORKFLOWS\",\n      \"RISKS_ETHICS_AND_VALIDITY\",\n      \"METHODS_AND_APPLICATIONS\"\n    ],\n    \"Anna Jean Wirth\": [\n      \"METHODS_AND_APPLICATIONS\",\n      \"DATA_COLLECTION_AND_QUALITY\",\n      \"CUI_AND_DOMAIN_SPECIFIC_LANGUAGE\",\n      \"LIT_REVIEW_AND_SEARCH_AUTOMATION\",\n      \"TOOLS_AND_WORKFLOWS\",\n      \"ADOPTION_BARRIERS_AND_STRATEGIES\"\n    ]\n  }\n}\n\nPHASE 3: ENTITY AND RELATIONSHIP MAPPING\n{\n  \"entities\": [\n    \"RAND\",\n    \"Methods Center\",\n    \"Social Network Analysis\",\n    \"AI / LLMs\",\n    \"Document parsing (doctrine / policy -> network)\",\n    \"Interview coding (deduce, Muse, Excel workflows)\",\n    \"Literature review / search automation\",\n    \"Wargaming / Game design\",\n    \"R\",\n    \"Python\",\n    \"Gephi\",\n    \"Controlled Unclassified Information (CUI)\",\n    \"People-finder / central repository\",\n    \"Sponsors / Clients\",\n    \"Adoption / Trust / QA (hallucination risk)\"\n  ],\n  \"relationships\": [\n    {\n      \"entity_1\": \"Social Network Analysis\",\n      \"entity_2\": \"R\",\n      \"relationship_type\": \"uses\",\n      \"strength\": 0.9,\n      \"supporting_evidence\": [\n        \"Some visualization that is done in Geffe, a lot of the heavy duty analysis, 70 percent in R and 30 percent in python.\"\n      ]\n    },\n    {\n      \"entity_1\": \"Social Network Analysis\",\n      \"entity_2\": \"Python\",\n      \"relationship_type\": \"uses\",\n      \"strength\": 0.85,\n      \"supporting_evidence\": [\n        \"Some visualization that is done in Geffe, a lot of the heavy duty analysis, 70 percent in R and 30 percent in python.\"\n      ]\n    },\n    {\n      \"entity_1\": \"AI / LLMs\",\n      \"entity_2\": \"Document parsing (doctrine / policy -> network)\",\n      \"relationship_type\": \"enables\",\n      \"strength\": 0.85,\n      \"supporting_evidence\": [\n        \"The second method above ... is semi automatable. The challenge is telling AI what the signal looks like and what it should look for and how to code it.\",\n        \"He used a 3-30 document that described the procedure for the air operations center... The first step is figuring out who all the actors are... he did it just entirely by looking at what was written in doctrine. The first step from text documents which presumably you could automate.\"\n      ]\n    },\n    {\n      \"entity_1\": \"Methods Center\",\n      \"entity_2\": \"People-finder / central repository\",\n      \"relationship_type\": \"seeks to build\",\n      \"strength\": 0.9,\n      \"supporting_evidence\": [\n        \"So goal is to create a community of folks to cross pollenate.\",\n        \"Internally it would be useful to be able to identify which RAND products are using network analysis and who is doing it.\"\n      ]\n    },\n    {\n      \"entity_1\": \"Fragmented internal resources (RAND intranet / folders)\",\n      \"entity_2\": \"Researcher discovery / reuse\",\n      \"relationship_type\": \"constrains\",\n      \"strength\": 0.9,\n      \"supporting_evidence\": [\n        \"There is a very diverse set of people that use the method across RAND and people are not always in conversation with each other, calling it different things and not always in conversation with each other.\",\n        \"there's two different pages on the RAND Intranet site that tell you what RAND's AI resources are like.\",\n        \"I sometimes go to other people's project folders and I'm like, oh, they have... I sometimes go to other people's project folders and I'm like, oh, they have.\"\n      ]\n    },\n    {\n      \"entity_1\": \"Controlled Unclassified Information (CUI)\",\n      \"entity_2\": \"Off-the-shelf AI / LLMs\",\n      \"relationship_type\": \"constrains\",\n      \"strength\": 0.9,\n      \"supporting_evidence\": [\n        \"At least half of the time, at least one of those documents that we get will be cui.\",\n        \"The language used by the military can be different. It is its own thing, so it can be a little bit hard to do that with some of the off the shelf AI tools.\"\n      ]\n    },\n    {\n      \"entity_1\": \"Interview coding (deduce, Muse, Excel workflows)\",\n      \"entity_2\": \"AI / LLMs\",\n      \"relationship_type\": \"can be augmented by\",\n      \"strength\": 0.85,\n      \"supporting_evidence\": [\n        \"coding would be really helpful... For just coding large bodies of interview notes.\",\n        \"I've seen some efforts even within sort of the the current platforms like deduce to AI it up but I don't know where they stand with it.\"\n      ]\n    },\n    {\n      \"entity_1\": \"People-finder / central repository\",\n      \"entity_2\": \"Discoverability of methods and practitioners\",\n      \"relationship_type\": \"improves\",\n      \"strength\": 0.9,\n      \"supporting_evidence\": [\n        \"If we could put all the ISDP reports into an AI and the AI tells us what methods we use.\",\n        \"There is a people Finder or whatever we call that has actually read everybody's research and incorporates the kinds of work they actually do.\"\n      ]\n    },\n    {\n      \"entity_1\": \"Adoption / Trust / QA (hallucination risk)\",\n      \"entity_2\": \"AI uptake among researchers\",\n      \"relationship_type\": \"constrains\",\n      \"strength\": 0.9,\n      \"supporting_evidence\": [\n        \"Hallucination.\",\n        \"I just don't think... I don't think it's trustworthy and I don't think it's capable of original thought.\",\n        \"I think if we actually have tools that are highly functional and truly deliver. People are gonna catch on pretty quickly.\"\n      ]\n    },\n    {\n      \"entity_1\": \"Data collection and coding (network studies)\",\n      \"entity_2\": \"Scale / centrality of Social Network Analysis in projects\",\n      \"relationship_type\": \"limits\",\n      \"strength\": 0.95,\n      \"supporting_evidence\": [\n        \"Ultimately once you commit to doing network analysis the biggest issue is collecting and coding the data.\",\n        \"The question is with additional tools if you lowered the barrier whether you could move that (more casual use of social network analysis into a more focused approach).\"\n      ]\n    },\n    {\n      \"entity_1\": \"AI / LLMs\",\n      \"entity_2\": \"Literature review / search automation\",\n      \"relationship_type\": \"augments\",\n      \"strength\": 0.9,\n      \"supporting_evidence\": [\n        \"even if you start with like a human driven literature review, maybe you then have the AI like trying to figure out what you might have missed.\",\n        \"We ran LLM on five years worth of reports to look at methods.\"\n      ]\n    },\n    {\n      \"entity_1\": \"Wargaming / Game design\",\n      \"entity_2\": \"AI / LLMs\",\n      \"relationship_type\": \"can be augmented by\",\n      \"strength\": 0.8,\n      \"supporting_evidence\": [\n        \"I want AI assisted adjudication or having AI assist with player roles... I can have AI as a player when I don't have the capacity to have a real person here.\",\n        \"It's like having a... I want to build a game... I can have the AI play this lots of times with slightly different variables or different inputs... look at that as a mechanism of robustness.\"\n      ]\n    }\n  ],\n  \"cause_effect_chains\": [\n    \"If AI automates routine survey workflows (sending emails, ingesting responses) -> manual coding effort falls -> barrier to using social network analysis is lowered -> more projects can make network analysis central ('The first method described above is automatable because it involves sending out emails and getting responses in structured format.'; 'could take a large number of projects using network analysis in casual way and convert them into projects where it is the central part of the analysis.').\",\n    \"If document-parsing AI identifies actors/relations in doctrine -> textual procedures can be converted into structured network data without manual hand-coding -> enables process-to-network studies at scale ('He used a 3-30 document... The first step is figuring out who all the actors are... he did it just entirely by looking at what was written in doctrine. The first step from text documents which presumably you could automate.').\",\n    \"Fragmentation of internal resources and multiple, inconsistent repositories -> researchers cannot discover past products or practitioners easily -> duplication of effort and slower cross-pollination ('There is a very diverse set of people that use the method across RAND and people are not always in conversation with each other'; 'there's two different pages on the RAND Intranet site').\",\n    \"Prevalence of CUI and service-specific jargon -> off-the-shelf LLMs perform poorly or risk data exposure -> need for CUI-aware, domain-specific models and governance ('At least half of the time, at least one of those documents that we get will be cui.'; 'The language used by the military can be different. It is its own thing, so it can be a little bit hard to do that with some of the off the shelf AI tools.').\",\n    \"Concerns about hallucination and validity -> require QA/guardrails and early wins on low-risk automation (formatting, templates, people-finder) -> builds trust and drives adoption ('Hallucination.'; 'If you build it, they'll come.'; 'start with low-hanging fruit like formatting, templates, reproducible snippets').\"\n  ],\n  \"conceptual_connections\": [\n    \"AI opportunities cut across literature reviews, interview coding, document parsing, and dissemination (podcasts/games) — a single class of tools can target multiple method bottlenecks.\",\n    \"Data sensitivity (CUI) and domain-specific language requirements link technical feasibility to governance and platform choice (internal RAND tools vs. external providers).\",\n    \"Knowledge-sharing (people-finder / repository) connects adoption strategy, discovery, and method reuse — improving discoverability reduces redundant work and enables cross-unit methods integration.\",\n    \"Adoption strategy ties trust (QA, low-risk wins) to uptake: demonstrating reliable productivity gains (templates, formatting, citation support) drives broader researcher use.\",\n    \"Method heterogeneity (different analyses, bespoke models) implies task- and domain-specific automation is more realistic than one-size-fits-all solutions.\"\n  ]\n}\n\nPHASE 4: SYNTHESIS AND FINAL ANALYSIS\n{\n  \"executive_summary\": \"Researchers across RAND frequently identified AI as valuable for routine, time-consuming research tasks (literature reviews, document search, formatting, transcription/summaries, and initial coding) while stressing that qualitative engagement with subject-matter experts remains essential. Key constraints are fragmented internal discovery, heterogeneous tooling, data collection/coding bottlenecks (especially for social network analysis), and strong CUI/domain-language and hallucination/QA concerns that argue for domain-specific, CUI-capable pilots and careful human-in-the-loop governance.\",\n  \"key_findings\": [\n    \"AI is seen as useful for accelerating routine, time-consuming tasks: Lisa Saum-Manning: \\\"a lot of literature reviews, most of our work is heavily qualitative... that is an area where AI could be well applied.\\\"\",\n    \"Interview coding and qualitative synthesis are repeatedly raised as ripe for augmentation: Lisa Saum-Manning: \\\"coding would be really helpful... For just coding large bodies of interview notes.\\\"\",\n    \"Document-to-network extraction is a concrete, interview-identified opportunity: Matthew Sergant described Jon Fujimora's manual approach and said, \\\"The first step from text documents which presumably you could automate.\\\"\",\n    \"Data collection and coding remain the primary bottleneck for scaling social network analysis: Matthew Sergant: \\\"Ultimately once you commit to doing network analysis the biggest issue is collecting and coding the data.\\\"\",\n    \"Internal discovery is fragmented and slows reuse; interviewees want an AI-backed people-finder/repository: Christopher Mouton and Heather Williams noted scattered internal resources; Heather suggested, \\\"If we could put all the ISDP reports into an AI and the AI tells us what methods we use.\\\"\",\n    \"CUI and service-specific language materially constrain use of off-the-shelf LLMs: Anna Jean Wirth: \\\"At least half of the time, at least one of those documents that we get will be cui.\\\" and \\\"The language used by the military can be different. It is its own thing.\\\"\",\n    \"Adoption should start with pragmatic, low-risk productivity wins to build trust: Christopher Mouton recommended focusing on formatting/templates and said, \\\"If you build it, they'll come.\\\"\",\n    \"Hallucination and validity concerns require explicit QA and human oversight: multiple speakers cautioned about errors — Lisa Saum-Manning: \\\"Hallucination.\\\" and Heather Williams: \\\"I just don't think... I don't think it's trustworthy and I don't think it's capable of original thought.\\\"\",\n    \"Wargaming and simulation were identified as promising AI augmentation areas (adjudication, AI players, running many runs): Heather Williams: \\\"I want AI assisted adjudication or having AI assist with player roles... I can have AI as a player when I don't have the capacity to have a real person here.\\\"\",\n    \"Tooling and workflow heterogeneity (R/Python/Gephi/deduce/Excel) means reusable, modular code/snippets are preferable to one-size-fits-all automation: Matthew Sergant noted, \\\"a lot of the heavy duty analysis, 70 percent in R and 30 percent in python,\\\" and advocated for \\\"standard set of R libraries... snippets of code you can use.\\\"\"\n  ],\n  \"cross_cutting_patterns\": [\n    \"AI applicability cuts across multiple method pain points (literature review, interview coding, document parsing, dissemination), so one class of capabilities can address many teams' needs.\",\n    \"Domain/CUI requirements repeatedly connect technical feasibility to platform choice and governance; off-the-shelf models often underperform on military doctrine/TTPS.\",\n    \"Fragmentation of internal knowledge (multiple intranets, scattered project folders) limits discovery and reuse, creating repeated, avoidable work.\",\n    \"Researchers favor pragmatic, visible productivity gains (templates, formatting, people-finder, reproducible code) as trust-building steps before more ambitious analytic automation.\",\n    \"Data collection and coding bottlenecks — not analytic algorithms — are the main constraint on scaling methods like social network analysis.\",\n    \"Heterogeneous tools and bespoke analyses across RAND imply that modular, task-specific automation (snippets, adapters, domain-tuned models) will be more adoptable than universal solutions.\"\n  ],\n  \"actionable_recommendations\": [\n    {\n      \"title\": \"Pilot a CUI-aware document parser that converts doctrine/policy into structured actor-relation outputs\",\n      \"description\": \"Build a prototype that ingests sponsor-provided doctrine/TTPS/policy and extracts actors, interactions, and data flows into structured (CSV/graph) outputs suitable for social network analysis. Scope the pilot on a narrow, well-documented process (e.g., an air operations center 3-30 document) and pair model outputs with a human validation step to measure coding accuracy. Use an internal hosting solution (RandChat or equivalent) for CUI handling, and include explicit signal definitions so the parser knows what to extract — reflecting Matthew Sergant's example: \\\"He used a 3-30 document... The first step is figuring out who all the actors are... he did it just entirely by looking at what was written in doctrine. The first step from text documents which presumably you could automate.\\\"\",\n      \"priority\": \"high\",\n      \"supporting_themes\": [\n        \"DOCUMENT_PARSING_TO_NETWORK\",\n        \"CUI_AND_DOMAIN_SPECIFIC_LANGUAGE\",\n        \"AI_OPPORTUNITIES\",\n        \"SOCIAL_NETWORK_ANALYSIS_SPECIFICS\",\n        \"DATA_COLLECTION_AND_QUALITY\"\n      ]\n    },\n    {\n      \"title\": \"Develop an AI-backed people-finder / central repository for methods and products\",\n      \"description\": \"Create a searchable index that ingests RAND products, project metadata, and methods statements to surface who used which methods and related deliverables. Start by indexing unclassified ISDP reports as Heather Williams suggested: \\\"If we could put all the ISDP reports into an AI and the AI tells us what methods we use.\\\" Ensure the prototype can be searched by unit, method, sponsor, and deliverable type and provide simple exportable metadata to speed proposal scoping and prevent duplicated work.\",\n      \"priority\": \"high\",\n      \"supporting_themes\": [\n        \"KNOWLEDGE_SHARING_REPOSITORY_NEEDS\",\n        \"COMMUNITY_AND_ORGANIZATIONAL_ISSUES\",\n        \"TOOLS_AND_WORKFLOWS\",\n        \"ADOPTION_BARRIERS_AND_STRATEGIES\"\n      ]\n    },\n    {\n      \"title\": \"Deliver a 'low-risk productivity' toolkit (formatting, citation, template automation, reusable R/Python snippets)\",\n      \"description\": \"Implement internal services that automate repetitive publication tasks: PD/front-matter population, document formatting with track changes, citation checks, and a curated snippet library for standard visualizations and first-cut publication-quality graphs (R/Python). Christopher Mouton recommended these as quick wins: \\\"If you build it, they'll come,\\\" and noted value in automating front-matter and formatting. Pilot the toolkit with a few volunteer teams and measure time savings and error reduction.\",\n      \"priority\": \"high\",\n      \"supporting_themes\": [\n        \"TOOLS_AND_WORKFLOWS\",\n        \"R_PYTHON_AND_SOFTWARE_PRACTICES\",\n        \"ADOPTION_BARRIERS_AND_STRATEGIES\"\n      ]\n    },\n    {\n      \"title\": \"Pilot AI-augmented interview coding with rigorous human-in-the-loop QA\",\n      \"description\": \"Run a controlled pilot where deduce/Muse or an internal LLM assists coding of a bounded corpus of interview transcripts; require human validation of model-suggested codes, track mismatches, and produce an audit trail for HSPC/CUI compliance. Ground the pilot in Lisa Saum-Manning's need: \\\"coding would be really helpful... For just coding large bodies of interview notes,\\\" while explicitly addressing hallucination and validity concerns by building QA checks into the workflow.\",\n      \"priority\": \"medium\",\n      \"supporting_themes\": [\n        \"INTERVIEW_CODING_AUTOMATION\",\n        \"RISKS_ETHICS_AND_VALIDITY\",\n        \"DATA_COLLECTION_AND_QUALITY\",\n        \"TOOLS_AND_WORKFLOWS\"\n      ]\n    },\n    {\n      \"title\": \"Invest in domain-specific / service-adapted LLMs or adapters with CUI-safe hosting\",\n      \"description\": \"Rather than relying solely on off-the-shelf models, develop or fine-tune models (or adapter layers) trained on service-specific language and doctrine to improve reliability on military material. Host these within RAND-approved infrastructure to handle CUI. Anna Jean Wirth emphasized the need: \\\"the language used by the military can be different. It is its own thing...\\\" and recommended service-specific investments to accelerate adoption.\",\n      \"priority\": \"high\",\n      \"supporting_themes\": [\n        \"CUI_AND_DOMAIN_SPECIFIC_LANGUAGE\",\n        \"AI_OPPORTUNITIES\",\n        \"TOOLS_AND_WORKFLOWS\",\n        \"ADOPTION_BARRIERS_AND_STRATEGIES\"\n      ]\n    },\n    {\n      \"title\": \"Form a community-of-practice program with shareable training, 'success story' demos, and a snippets repository\",\n      \"description\": \"Create recurring demos, an internal AI circle or lunch-and-learn series, and a centrally maintained repository of validated templates, code snippets, and case studies. Christopher Mouton and others pointed to discovery and adoption gaps: \\\"There is a very diverse set of people that use the method across RAND... people are not always in conversation with each other.\\\" Use this program to surface functional tools and reduce stove-piping of good ideas.\",\n      \"priority\": \"medium\",\n      \"supporting_themes\": [\n        \"COMMUNITY_AND_ORGANIZATIONAL_ISSUES\",\n        \"KNOWLEDGE_SHARING_REPOSITORY_NEEDS\",\n        \"ADOPTION_BARRIERS_AND_STRATEGIES\"\n      ]\n    },\n    {\n      \"title\": \"Prototype AI augmentation for wargaming (adjudication, AI players, post-game synthesis) in controlled, unclassified pilots\",\n      \"description\": \"Run limited pilots that use AI as adjudicator or simulated players in wargames, with careful scopes and human oversight. Heather Williams suggested using AI to augment game runs and robustness testing: \\\"I can have the AI play this lots of times with slightly different variables... look at that as a mechanism of robustness.\\\" Pair these pilots with automated post-game synthesis tools to capture and disseminate insights.\",\n      \"priority\": \"medium\",\n      \"supporting_themes\": [\n        \"AI_OPPORTUNITIES\",\n        \"METHODS_AND_APPLICATIONS\",\n        \"TOOLS_AND_WORKFLOWS\"\n      ]\n    }\n  ],\n  \"confidence_assessment\": {\n    \"METHODS_AND_APPLICATIONS\": {\n      \"level\": \"high\",\n      \"score\": 0.9,\n      \"evidence\": \"\\\"Our unit is quite diverse... we use almost all the methods.\\\" (Matthew Sergant); Anna Jean Wirth: \\\"we use almost all the methods except for like the whole category of methods that are like really people centric.\\\"\"\n    },\n    \"AI_OPPORTUNITIES\": {\n      \"level\": \"high\",\n      \"score\": 0.9,\n      \"evidence\": \"\\\"That is a step that emerging AI and LLM is making possible and doable. This is the new thing that AI allows.\\\" (Matthew Sergant); Heather Williams: \\\"Even if you start with like a human driven literature review, maybe you then have the AI like trying to figure out what you might have missed.\\\"\"\n    },\n    \"DOCUMENT_PARSING_TO_NETWORK\": {\n      \"level\": \"medium-high\",\n      \"score\": 0.85,\n      \"evidence\": \"Matthew Sergant described manual coding of doctrine into networks and said: \\\"The first step from text documents which presumably you could automate.\\\"\"\n    },\n    \"INTERVIEW_CODING_AUTOMATION\": {\n      \"level\": \"high\",\n      \"score\": 0.8,\n      \"evidence\": \"Lisa Saum-Manning: \\\"coding would be really helpful... For just coding large bodies of interview notes.\\\" Todd and others noted tools like Muse/deduce are being explored.\"\n    },\n    \"LIT_REVIEW_AND_SEARCH_AUTOMATION\": {\n      \"level\": \"high\",\n      \"score\": 0.85,\n      \"evidence\": \"Multiple speakers: Lisa Saum-Manning: \\\"a lot of literature reviews... that is an area where AI could be well applied.\\\" Todd: \\\"We ran LLM on five years worth of reports to look at methods.\\\"\"\n    },\n    \"DATA_COLLECTION_AND_QUALITY\": {\n      \"level\": \"high\",\n      \"score\": 0.9,\n      \"evidence\": \"Matthew Sergant: \\\"Ultimately once you commit to doing network analysis the biggest issue is collecting and coding the data.\\\" and cause-effect analyses in Phase 3 emphasize data collection as a limit to scale.\"\n    },\n    \"CUI_AND_DOMAIN_SPECIFIC_LANGUAGE\": {\n      \"level\": \"high\",\n      \"score\": 0.9,\n      \"evidence\": \"Anna Jean Wirth: \\\"At least half of the time, at least one of those documents that we get will be cui.\\\" and \\\"The language used by the military can be different. It is its own thing.\\\"\"\n    },\n    \"TOOLS_AND_WORKFLOWS\": {\n      \"level\": \"high\",\n      \"score\": 0.9,\n      \"evidence\": \"Multiple mentions of R, Python, Gephi, deduce, Excel; Matthew: \\\"a lot of the heavy duty analysis, 70 percent in R and 30 percent in python.\\\" Christopher: desire for formatting/citation automation.\"\n    },\n    \"R_PYTHON_AND_SOFTWARE_PRACTICES\": {\n      \"level\": \"medium-high\",\n      \"score\": 0.8,\n      \"evidence\": \"Matthew Sergant: \\\"a lot of the heavy duty analysis, 70 percent in R and 30 percent in python.\\\" and calls for shareable R libraries/snippets.\"\n    },\n    \"COMMUNITY_AND_ORGANIZATIONAL_ISSUES\": {\n      \"level\": \"high\",\n      \"score\": 0.85,\n      \"evidence\": \"Christopher Mouton and Matthew described fragmentation: \\\"There is a very diverse set of people... not always in conversation with each other.\\\" Heather: multiple intranet pages with different resources.\"\n    },\n    \"KNOWLEDGE_SHARING_REPOSITORY_NEEDS\": {\n      \"level\": \"high\",\n      \"score\": 0.8,\n      \"evidence\": \"Heather Williams: \\\"If we could put all the ISDP reports into an AI...\\\" and Christopher: desire for a people-finder that has read everybody's research.\"\n    },\n    \"ADOPTION_BARRIERS_AND_STRATEGIES\": {\n      \"level\": \"high\",\n      \"score\": 0.8,\n      \"evidence\": \"Christopher Mouton: \\\"start with low-hanging fruit like formatting, templates, reproducible snippets\\\"; multiple speakers emphasized trust-building through reliable tools.\"\n    },\n    \"RISKS_ETHICS_AND_VALIDITY\": {\n      \"level\": \"high\",\n      \"score\": 0.9,\n      \"evidence\": \"Direct cautions: \\\"Hallucination.\\\" (Lisa); Heather: \\\"I just don't think... I don't think it's trustworthy.\\\" and concerns about de-identification and incorrect citations raised across interviews.\"\n    },\n    \"SOCIAL_NETWORK_ANALYSIS_SPECIFICS\": {\n      \"level\": \"medium\",\n      \"score\": 0.75,\n      \"evidence\": \"Matthew provided concrete workflow examples (survey batteries, doctrine-to-network coding) and community estimates: \\\"About 100.\\\" He emphasized the data/coding constraint and semi-automatable opportunities.\"\n    }\n  }\n}\n\n=== END OF ANALYSIS ===",
    "key_relationships": [
      {
        "entities": "Social Network Analysis -> R",
        "strength": 0.9,
        "type": "uses"
      },
      {
        "entities": "Social Network Analysis -> Python",
        "strength": 0.85,
        "type": "uses"
      },
      {
        "entities": "AI / LLMs -> Document parsing (doctrine / policy -> network)",
        "strength": 0.85,
        "type": "enables"
      },
      {
        "entities": "Methods Center -> People-finder / central repository",
        "strength": 0.9,
        "type": "seeks to build"
      },
      {
        "entities": "Fragmented internal resources (RAND intranet / folders) -> Researcher discovery / reuse",
        "strength": 0.9,
        "type": "constrains"
      },
      {
        "entities": "Controlled Unclassified Information (CUI) -> Off-the-shelf AI / LLMs",
        "strength": 0.9,
        "type": "constrains"
      },
      {
        "entities": "Interview coding (deduce, Muse, Excel workflows) -> AI / LLMs",
        "strength": 0.85,
        "type": "can be augmented by"
      },
      {
        "entities": "People-finder / central repository -> Discoverability of methods and practitioners",
        "strength": 0.9,
        "type": "improves"
      },
      {
        "entities": "Adoption / Trust / QA (hallucination risk) -> AI uptake among researchers",
        "strength": 0.9,
        "type": "constrains"
      },
      {
        "entities": "Data collection and coding (network studies) -> Scale / centrality of Social Network Analysis in projects",
        "strength": 0.95,
        "type": "limits"
      },
      {
        "entities": "AI / LLMs -> Literature review / search automation",
        "strength": 0.9,
        "type": "augments"
      },
      {
        "entities": "Wargaming / Game design -> AI / LLMs",
        "strength": 0.8,
        "type": "can be augmented by"
      }
    ],
    "key_themes": [
      "AI is seen as useful for accelerating routine, time-consuming tasks: Lisa Saum-Manning: \"a lot of literature reviews, most of our work is heavily qualitative... that is an area where AI could be well applied.\"",
      "Interview coding and qualitative synthesis are repeatedly raised as ripe for augmentation: Lisa Saum-Manning: \"coding would be really helpful... For just coding large bodies of interview notes.\"",
      "Document-to-network extraction is a concrete, interview-identified opportunity: Matthew Sergant described Jon Fujimora's manual approach and said, \"The first step from text documents which presumably you could automate.\"",
      "Data collection and coding remain the primary bottleneck for scaling social network analysis: Matthew Sergant: \"Ultimately once you commit to doing network analysis the biggest issue is collecting and coding the data.\"",
      "Internal discovery is fragmented and slows reuse; interviewees want an AI-backed people-finder/repository: Christopher Mouton and Heather Williams noted scattered internal resources; Heather suggested, \"If we could put all the ISDP reports into an AI and the AI tells us what methods we use.\"",
      "CUI and service-specific language materially constrain use of off-the-shelf LLMs: Anna Jean Wirth: \"At least half of the time, at least one of those documents that we get will be cui.\" and \"The language used by the military can be different. It is its own thing.\"",
      "Adoption should start with pragmatic, low-risk productivity wins to build trust: Christopher Mouton recommended focusing on formatting/templates and said, \"If you build it, they'll come.\"",
      "Hallucination and validity concerns require explicit QA and human oversight: multiple speakers cautioned about errors — Lisa Saum-Manning: \"Hallucination.\" and Heather Williams: \"I just don't think... I don't think it's trustworthy and I don't think it's capable of original thought.\"",
      "Wargaming and simulation were identified as promising AI augmentation areas (adjudication, AI players, running many runs): Heather Williams: \"I want AI assisted adjudication or having AI assist with player roles... I can have AI as a player when I don't have the capacity to have a real person here.\"",
      "Tooling and workflow heterogeneity (R/Python/Gephi/deduce/Excel) means reusable, modular code/snippets are preferable to one-size-fits-all automation: Matthew Sergant noted, \"a lot of the heavy duty analysis, 70 percent in R and 30 percent in python,\" and advocated for \"standard set of R libraries... snippets of code you can use.\""
    ],
    "model_used": "gpt-5-mini",
    "processing_time_seconds": 290.4,
    "recommendations": [
      {
        "description": "Build a prototype that ingests sponsor-provided doctrine/TTPS/policy and extracts actors, interactions, and data flows into structured (CSV/graph) outputs suitable for social network analysis. Scope the pilot on a narrow, well-documented process (e.g., an air operations center 3-30 document) and pair model outputs with a human validation step to measure coding accuracy. Use an internal hosting solution (RandChat or equivalent) for CUI handling, and include explicit signal definitions so the parser knows what to extract — reflecting Matthew Sergant's example: \"He used a 3-30 document... The first step is figuring out who all the actors are... he did it just entirely by looking at what was written in doctrine. The first step from text documents which presumably you could automate.\"",
        "priority": "high",
        "title": "Pilot a CUI-aware document parser that converts doctrine/policy into structured actor-relation outputs"
      },
      {
        "description": "Create a searchable index that ingests RAND products, project metadata, and methods statements to surface who used which methods and related deliverables. Start by indexing unclassified ISDP reports as Heather Williams suggested: \"If we could put all the ISDP reports into an AI and the AI tells us what methods we use.\" Ensure the prototype can be searched by unit, method, sponsor, and deliverable type and provide simple exportable metadata to speed proposal scoping and prevent duplicated work.",
        "priority": "high",
        "title": "Develop an AI-backed people-finder / central repository for methods and products"
      },
      {
        "description": "Implement internal services that automate repetitive publication tasks: PD/front-matter population, document formatting with track changes, citation checks, and a curated snippet library for standard visualizations and first-cut publication-quality graphs (R/Python). Christopher Mouton recommended these as quick wins: \"If you build it, they'll come,\" and noted value in automating front-matter and formatting. Pilot the toolkit with a few volunteer teams and measure time savings and error reduction.",
        "priority": "high",
        "title": "Deliver a 'low-risk productivity' toolkit (formatting, citation, template automation, reusable R/Python snippets)"
      },
      {
        "description": "Run a controlled pilot where deduce/Muse or an internal LLM assists coding of a bounded corpus of interview transcripts; require human validation of model-suggested codes, track mismatches, and produce an audit trail for HSPC/CUI compliance. Ground the pilot in Lisa Saum-Manning's need: \"coding would be really helpful... For just coding large bodies of interview notes,\" while explicitly addressing hallucination and validity concerns by building QA checks into the workflow.",
        "priority": "medium",
        "title": "Pilot AI-augmented interview coding with rigorous human-in-the-loop QA"
      },
      {
        "description": "Rather than relying solely on off-the-shelf models, develop or fine-tune models (or adapter layers) trained on service-specific language and doctrine to improve reliability on military material. Host these within RAND-approved infrastructure to handle CUI. Anna Jean Wirth emphasized the need: \"the language used by the military can be different. It is its own thing...\" and recommended service-specific investments to accelerate adoption.",
        "priority": "high",
        "title": "Invest in domain-specific / service-adapted LLMs or adapters with CUI-safe hosting"
      },
      {
        "description": "Create recurring demos, an internal AI circle or lunch-and-learn series, and a centrally maintained repository of validated templates, code snippets, and case studies. Christopher Mouton and others pointed to discovery and adoption gaps: \"There is a very diverse set of people that use the method across RAND... people are not always in conversation with each other.\" Use this program to surface functional tools and reduce stove-piping of good ideas.",
        "priority": "medium",
        "title": "Form a community-of-practice program with shareable training, 'success story' demos, and a snippets repository"
      },
      {
        "description": "Run limited pilots that use AI as adjudicator or simulated players in wargames, with careful scopes and human oversight. Heather Williams suggested using AI to augment game runs and robustness testing: \"I can have the AI play this lots of times with slightly different variables... look at that as a mechanism of robustness.\" Pair these pilots with automated post-game synthesis tools to capture and disseminate insights.",
        "priority": "medium",
        "title": "Prototype AI augmentation for wargaming (adjudication, AI players, post-game synthesis) in controlled, unclassified pilots"
      }
    ],
    "single_speaker_mode": false,
    "speakers_identified": [
      {
        "name": "Matthew Sergant",
        "perspective": "Sees social network analysis as widely embedded across RAND with recurring barriers in data collection/coding and knowledge sharing. Believes AI can meaningfully automate routinized steps (survey workflows, document-to-network extraction) if signal definitions are clear, but adoption depends on demonstrable, trustworthy tools and realistic sponsor expectations.",
        "role": "RAND Research Methods Center — Social Network Analysis"
      },
      {
        "name": "Heather Williams",
        "perspective": "Values AI for augmenting wargaming (adjudication, player simulation), literature review augmentation, and dissemination (e.g., podcast generation), but is cautious about over-reliance — questions trust, originality, and organizational fragmentation of tools and resources.",
        "role": "ISDP Researcher (wargaming and qualitative methods)"
      },
      {
        "name": "Lisa Saum-Manning",
        "perspective": "Strongly supports AI to speed literature reviews, automate interview coding and generate dissemination products; emphasizes the need for QA, validation, and guardrails to avoid hallucination and maintain trust in outputs. Wants AI to broaden perspective (business literature, outside lessons) to improve actionable recommendations.",
        "role": "ISDP Researcher (qualitative methods & dissemination)"
      },
      {
        "name": "Christopher Mouton",
        "perspective": "Argues for prioritizing high-leverage, low-risk AI applications (document formatting, people-finder, reproducible snippets) that reduce friction and demonstrate value quickly. Sees community fragmentation and discovery problems as major barriers and warns about hallucination risk if tools are not well-governed.",
        "role": "ISDP Senior Researcher / Methods advocate"
      },
      {
        "name": "Anna Jean Wirth",
        "perspective": "Describes wide-ranging methods in her program and highlights that most projects require qualitative engagement to scope problems. Emphasizes practical barriers: military/domain-specific language, frequent CUI, and variability of documents. Recommends investment in CUI-ready, service-specific AI tools for document parsing and search to accelerate research.",
        "role": "Resource Management Program, Project Air Force (Program Lead / Researcher)"
      }
    ],
    "total_codes": 14,
    "total_interviews": 3
  },
  "started_at": "2026-02-11T14:29:55.568191",
  "status": "completed"
}